{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize semantic change trajectory with diachronic word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see how to visualize the semantic trajectory of a word across time slices, based on the changes in nearest neighbours from one time slice to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It goes without saying but, before starting, make sure you have trained and aligned diachronic embedding models. We assume that the embeddings were trained on OCR'd texts and they therefore contain OCR errors, which is why the `spellchecker` package is applied to attempt automatically reducing the number of mispellings showing up among the nearest neighbours. As we will see below, this step will likely not be enough to exclude all OCR errors, which is why a further (optional) step for manual cleaning will be introduced before the very last step that generates the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = \"test1_BNF\"\n",
    "time_slices = [\"41765\", \"55686\", \"77146\", \"78302\", \"129439\", \"178772\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subdir for all outputs of this notebook if it does not exist already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./outputs'):\n",
    "    os.mkdir('./outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained diachronic embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following to the path to the folder containing the aligned diachonic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtomodels = f'../../outputs/{test_name}/aligned'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all models in the folder and sort them (so that we can identify the most recent time slice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "allmodels = sorted(glob(f'{pathtomodels}/*.model'))\n",
    "# if using the .txt files with only the vectors:\n",
    "#allmodels = sorted(glob(f'{pathtomodels}/*-vectors.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model for the most recent time slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastmodel = Word2Vec.load(allmodels[-1])\n",
    "# if using the .txt files with only the vectors:\n",
    "#lastmodel = KeyedVectors.load_word2vec_format(allmodels[-1], binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the 'name' of each time slice (retrieved from the name of the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_names = []\n",
    "for i in range(len(allmodels)):\n",
    "    slice_name = allmodels[i].split('/')[-1].split('-vectors.txt')[0]\n",
    "    slice_name = allmodels[i].split('/')[-1].split('.model')[0]\n",
    "    slices_names.append(slice_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 30.07it/s]\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for model in tqdm(allmodels):\n",
    "    modeltoappend = Word2Vec.load(model)\n",
    "    # if using the .txt files with only the vectors:\n",
    "    # modeltoappend = KeyedVectors.load_word2vec_format(model, binary=False)\n",
    "    models.append(modeltoappend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare timeslices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended that a maximum of 4 timeslices are compared in the visualization. With more than that, you might find the plot cluttered and thus uninformative. Before visualizing the trajectory, it might be useful to run a change point detection algorithm, to check when a word might have undergone meaning change. You can check out the notebook [`changepoint_detection.ipynb`](./changepoint_detection.ipynb) if you'd like to try that first. Once you have identified potential change points, in order to get the visually clearest representation of the trajectory, you might want to try different time slices for the variable `timeslicestocompare`. For example, if your change point detection algorithm detected _1810s_ as a potential change point, you could try including _1800s_, _1820s_, as well as one or two time slices (if available) which are further away in time (before or after) from the detected change points (e.g. _1780s_ and _1850s_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, change `keyword` to the word whose trajectory you wish to visualize. Then change `timeslicestocompare` to the relevant slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'vache'\n",
    "\n",
    "timeslicestocompare = [\"178772\"]\n",
    "\n",
    "timeslicesindeces = []\n",
    "for slice in timeslicestocompare:\n",
    "    index = slices_names.index(slice)\n",
    "    timeslicesindeces.append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the variable `topn` to how many nearest neighbours you'd like to include in the map. Note that eventually some of the _k_-nearest neighbours may not appear in the final map, as we will only visualize the union of all the nearest neighbours of the keyword across all time slices in `timeslicestocompare`, so that if a word is not in the vocabulary of all the time slices, it will be excluded. You will be able to check which of these have not been included by looking at the list `notpresent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting from 178772...\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for index in timeslicesindeces:\n",
    "    print(f'Extracting from {slices_names[index]}...')\n",
    "    keywordsim = models[index].wv.most_similar(positive=keyword, topn=topn)\n",
    "    # if using the .txt files with only the vectors:\n",
    "    # keywordsim = models[index].most_similar(positive=keyword, topn=topn)\n",
    "    for i in keywordsim:\n",
    "        vocab.append(i[0])\n",
    "vocab = list(dict.fromkeys(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `spellchecker`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvocab = []\n",
    "known = list(spell.known(vocab))\n",
    "for word in vocab:\n",
    "    if word in known:\n",
    "        newvocab.append(word)\n",
    "    else:\n",
    "        item2 = spell.correction(word)\n",
    "        if item2 != word:\n",
    "            newvocab.append(item2)\n",
    "\n",
    "newvocab = list(dict.fromkeys(newvocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell lets you add any word which might not be included in the nearest 20 neighbours, but which you might wish to include in the map anyway. Uncomment and add your words in the list `bespoke` if you want to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bespoke = ['boeuf']\n",
    "\n",
    "newvocab = newvocab + bespoke\n",
    "newvocab = list(dict.fromkeys(newvocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, if there are words or OCR errors not captured by `spellchecker` that you wish to manually remove from the map, uncomment and add those words in the list `tbr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbr = ['offic', 'whilst', 'true', 'tree', 'shows']\n",
    "for t in tbr:\n",
    "    if t in newvocab:\n",
    "        newvocab.remove(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "fortsne = []\n",
    "notpresent = []\n",
    "\n",
    "for word in newvocab:\n",
    "    if word != keyword:\n",
    "        try:\n",
    "            wordvecaverage = models[timeslicesindeces[-1]].wv[word]\n",
    "            # if using the .txt files with only the vectors:\n",
    "            # wordvecaverage = models[timeslicesindeces[-1]][word]\n",
    "            fortsne.append(wordvecaverage)\n",
    "        except KeyError:\n",
    "            notpresent.append(word)\n",
    "\n",
    "newvocab = [x for x in newvocab if x not in notpresent]\n",
    "\n",
    "for index in timeslicesindeces:\n",
    "    fortsne.append(models[index].wv[keyword])\n",
    "    # if using the .txt files with only the vectors:\n",
    "    # fortsne.append(models[index][keyword])\n",
    "for slicename in timeslicestocompare:\n",
    "    newvocab.append(f'{keyword}_{slicename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the vectors for all the words that we will include in the map, we will reduce their dimensions with [`TSNE`](https://lvdmaaten.github.io/tsne/). In the next cell, define some of the important parameters ([here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) for a complete list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "random_state = 19051992 # For reproducibility\n",
    "metric='euclidean'\n",
    "learning_rate='auto'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "perplexity must be less than n_samples",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(fortsne)\n\u001b[0;32m----> 3\u001b[0m X_embedded \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_embedded)\n\u001b[1;32m      7\u001b[0m df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(timeslicestocompare)]\n",
      "File \u001b[0;32m~/miniconda3/envs/BNF_diachronic/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:1122\u001b[0m, in \u001b[0;36mTSNE.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit X into an embedded space and return that transformed output.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \n\u001b[1;32m   1105\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m        Embedding of the training data in low-dimensional space.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params_vs_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X)\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ \u001b[38;5;241m=\u001b[39m embedding\n",
      "File \u001b[0;32m~/miniconda3/envs/BNF_diachronic/lib/python3.10/site-packages/sklearn/manifold/_t_sne.py:793\u001b[0m, in \u001b[0;36mTSNE._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_params_vs_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperplexity \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperplexity must be less than n_samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: perplexity must be less than n_samples"
     ]
    }
   ],
   "source": [
    "X = np.array(fortsne)\n",
    "\n",
    "X_embedded = TSNE(n_components=n_components, random_state=random_state, metric=metric, learning_rate=learning_rate).fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(X_embedded)\n",
    "\n",
    "df2 = df.iloc[:-len(timeslicestocompare)]\n",
    "df3 = df.iloc[-len(timeslicestocompare):]\n",
    "\n",
    "df3 = df3.reset_index()\n",
    "\n",
    "newvocab1 = newvocab[:-len(timeslicestocompare)]\n",
    "newvocab1\n",
    "newvocab2 = newvocab[-len(timeslicestocompare):]\n",
    "newvocab2\n",
    "\n",
    "x = df2[0] \n",
    "y = df2[1]\n",
    "x2 = df3[0]\n",
    "y2 = df3[1]\n",
    "\n",
    "# just some size adjustment...\n",
    "fig, ax = plt.subplots(1, figsize=(14,10),dpi=80,facecolor='#ffffff')\n",
    "ax.set_facecolor('#ffffff')\n",
    "\n",
    "# plot x and y\n",
    "plt.scatter(x,y,c=\"0.6\")\n",
    "\n",
    "textsvocab = [plt.text(x[i], y[i], txt,color='#71797E',fontsize = 14) for i,txt in enumerate(newvocab1)]\n",
    "adjust_text(textsvocab)\n",
    "\n",
    "plt.scatter(x2,y2,c=\"red\")\n",
    "\n",
    "textsvocab2 = [plt.text(x2[i], y2[i], txt,color='black',fontsize = 17) for i,txt in enumerate(newvocab2)]\n",
    "adjust_text(textsvocab2)\n",
    "\n",
    "def drawArrow(A, B):\n",
    "    plt.arrow(A[0], A[1], B[0] - A[0], B[1] - A[1], head_width=0.01, length_includes_head=True,color='black')\n",
    "\n",
    "for i in range(1,len(timeslicestocompare)):\n",
    "    X = np.array([x2[len(x2)-i],y2[len(y2)-i]])\n",
    "    Y = np.array([x2[len(x2)-(i+1)],y2[len(y2)-(i+1)]])\n",
    "    drawArrow(Y,X)\n",
    "\n",
    "#show the plot\n",
    "# plt.savefig(f'{keyword}.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bighistdiachemb",
   "language": "python",
   "name": "bighistdiachemb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54ed4be044bdbfddf339b8bedcd1bd34fa47dd0b44ee203d7e74423349b7c92b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
